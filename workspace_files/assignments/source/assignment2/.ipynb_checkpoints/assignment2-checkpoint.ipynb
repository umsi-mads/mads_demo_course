{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "version",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "version = \"REPLACE_PACKAGE_VERSION\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Design and Analysis\n",
    "## School of Information, University of Michigan\n",
    "\n",
    "## Week 2: \n",
    "\n",
    "\n",
    "## Assignment Overview\n",
    "### The objective of this assignment is to:\n",
    "\n",
    "- Apply theory of experiment design and knowledge of analysis techniques to real experiment data.\n",
    "\n",
    "\n",
    "### The total score of this assignment will be 19 points\n",
    "\n",
    "\n",
    "### Resources:\n",
    "- StatsModels, Scipy.stats, Numpy\n",
    "    - We recommend using two python libraries called [StatsModels](https://www.statsmodels.org/stable/index.html) and [Scipy.stats](https://docs.scipy.org/doc/scipy/reference/tutorial/stats.html) for data analysis. For this dataset, we'll be using [Numpy](https://numpy.org/devdocs/reference/index.html) as well.\n",
    "    \n",
    "- Optional Reading: [Holt C.A, & Laury S.K. Risk Aversion and Incentive Effects. (2002).](https://www.jstor.org/stable/3083270)  \n",
    "\n",
    "\n",
    "Datasets used in this assignment\n",
    "- Trust data [download csv file](assets/assignment2_data1.csv)\n",
    "- Fixed-Price Auction data [download csv file](assets/assignment2_data2.csv)\n",
    "    - Source for dataset: [Chen, Y., et al. Sealed bid auctions with ambiguity: Theory and experiments. (2007).](https://www.sciencedirect.com/science/article/pii/S0022053107000178)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "from scipy import stats\n",
    "#you may or may not use all of the above libraries, and that is OK!\n",
    "\n",
    "trust_data = pd.read_csv('assets/assignment2_data1.csv') #Trust Game data for this assignment\n",
    "fpa_data = pd.read_csv('assets/assignment2_data2.csv') #First-price auction data for this assignment - this is the same dataset from last week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment the below line to view readme files for this dataset (includes explanation of variable names)\n",
    "#!cat assets/assignment2_data1_readme.md\n",
    "#!cat assets/assignment2_data2_readme.md\n",
    "\n",
    "#uncomment the below line to view snippet of csv file\n",
    "#trust_data.head()\n",
    "#fpa_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "part_a_inst_text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Part A (3 points)\n",
    "\n",
    "1. For the Trust Game, \n",
    "subjects are grouped in pairs, paired with one assigned the role of an investor and another a recipient. Let's examine  the correlation between the amounts the investors invest and the amounts the recipients return. Complete the function below to return the correlation coefficient.\n",
    "\n",
    "**Round any calculations to the hundredth decimal. Do not use percentages.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "np_correlation_value",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def inv_rec_corrcoef(provided_data):\n",
    "    \"\"\" Later in this problem set, you will be modeling OLS regressions on your data. For now, we'll calculate just\n",
    "    the correlation coefficient using numpy. If needed, refer to the numpy documentation linked above.\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    invest = trust_data[trust_data['player role'] == 'first']['decision']\n",
    "    returner = trust_data[trust_data['player role'] == 'second']['decision']\n",
    "    answer = round(np.corrcoef(invest, returner)[0][1],2)\n",
    "    ### END SOLUTION\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "part_a_check_text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Your function should return a string with the correct coefficient value. Check that it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_rec_corrcoef(trust_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "correct_correlation_value1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(inv_rec_corrcoef(trust_data)) == np.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "correct_correlation_value2",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Part A #1: Checking value of correlation coefficient\"\"\"\n",
    "# Hidden tests\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert inv_rec_corrcoef(trust_data) == 0.82, \"Part A #1 correlation coefficient value differs\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "part_b_inst_text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "For the first-price auctions experiment, there are ten experimental sessions, with eight subjects per session. In this context, subjects are tasked with completing auction and lottery (Holt-Laury 2002) tasks in two orders. In five of the ten sessions, subjects first complete a lottery task, followed by 30 rounds of auctions. In the other five sessions, subjects first complete 30 rounds of auctions, followed by a lottery task. At the end of each session, subjects complete a demographics survey. The data sets extract the first period auction data for each treatment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "part_b_inst_text_1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In this case, say that the control for the first-price auction experiment is the order in which subjects complete the lottery task followed by the auction task (k1_8_lot_exp) and the outcome variable we want to measure is the bid-value ratio (b/v).\n",
    "\n",
    "1. Using differences-in-means, what is the average treatment effect for the first-price auction experiment? (4 points)\n",
    "\n",
    "**Round any calculations to the hundredth decimal. Do not use percentages.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "manual_ate_fpa",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def ate_fpa_payoff(provided_data):\n",
    "    \"\"\"\n",
    "    Write the function to manually check the differences in means of bid-value ratios across the different groups explained above.\n",
    "    Tip: the easiest way to do this is to create a new dataframe column called 'bidval_ratio' in the provided data.\n",
    "    Your function should output a dataframe with the following columns: 'lot_auc_mean', 'auc_lot_mean', 'diff in means'.\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    ate_fpa_payoff_df = pd.DataFrame(columns=['lot_auc_mean','auc_lot_mean', 'diff in means'])\n",
    "    provided_data['bidval_ratio'] = provided_data['b'] / provided_data['v']\n",
    "    ate_fpa_payoff_df['lot_auc_mean'] = [round(provided_data[provided_data['treatment'] == 'k1_8_lot_exp']['bidval_ratio'].mean(),2)]\n",
    "    ate_fpa_payoff_df['auc_lot_mean'] = [round(provided_data[provided_data['treatment'] == 'k1_8_exp_lot']['bidval_ratio'].mean(),2)]\n",
    "    ate_fpa_payoff_df['diff in means'] = ate_fpa_payoff_df['lot_auc_mean']- ate_fpa_payoff_df['auc_lot_mean']\n",
    "    ###END SOLUTION\n",
    "    return ate_fpa_payoff_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "part_b_check_text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Your function should return a dataframe with the correct values and columns. Check that it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ate_fpa_payoff(fpa_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ate_fpa_df",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance (ate_fpa_payoff(fpa_data), pd.core.frame.DataFrame), \"checking your data is in a dataframe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ate_fpa_df_columns",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert ate_fpa_payoff(fpa_data).columns[0] == 'lot_auc_mean', \"checking df column names\"\n",
    "assert ate_fpa_payoff(fpa_data).columns[1] == 'auc_lot_mean', \"checking df column names\"\n",
    "assert ate_fpa_payoff(fpa_data).columns[2] == 'diff in means', \"checking df column names\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ate_fpa_correct_value_lot",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"Part B: lot_auc_mean value\"\n",
    "# Hidden tests\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert next(iter(ate_fpa_payoff(fpa_data)[\"lot_auc_mean\"])) == round(fpa_data[fpa_data['treatment'] == 'k1_8_lot_exp']['bidval_ratio'].mean(),2), \"Part B #1 lot_auc_mean value differs\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ate_fpa_correct_value_auc",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"Part B: auc_lot_mean value\"\n",
    "# Hidden tests\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert next(iter(ate_fpa_payoff(fpa_data)['auc_lot_mean'])) == round(fpa_data[fpa_data['treatment'] == 'k1_8_exp_lot']['bidval_ratio'].mean(),2), \"Part B #1 auc_lot_mean value differs\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "part_c_inst_text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Continuing with the ```fpa_data``` dataset from last week, we would expect subjects to bid a certain fraction of their value in a first-price sealed bid auction depending on their risk attitudes (e.g., risk neutral, risk averse). Let's explore what effect gender has on bid-value ratios when controlled with risk. This time, let's calculate this average treatment effect using an ordinary least-squares regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "part_c_inst_text_1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "1. Using the ```fpa_data``` dataframe and an ordinary least-squares regression model, complete the ```ols_riskfemale_on_bidvalue``` function to evaluate how subjectsâ€™ risk attitudes and gender (in the form of the _female_ variable) affect their bid/value ratio. For now, we'll just return a summary view of our data. (2 points)\n",
    "\n",
    "**Round any calculations to the hundredth decimal. Do not use percentages.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ols_riskgender_summary",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def ols_riskgender_on_bidvalue(provided_data):\n",
    "    \n",
    "    \"\"\"\n",
    "    The easiest way to evaluate how subjects' risk attitudes and gender affect their bid/value ratio is to run an OLS linear\n",
    "    regression on your fpa_data dataframe. Use the statsmodels library to run an OLS linear regression, and return the summary\n",
    "    view of your results.\n",
    "\n",
    "    \"\"\"\n",
    "    X = provided_data[['female', 'ra']]\n",
    "    ### BEGIN SOLUTION\n",
    "    Y = provided_data['bv_ratio'] = provided_data['b'] / provided_data['v']\n",
    "    X = sm.add_constant(X)\n",
    "    model = sm.OLS(Y,X).fit()\n",
    "    model_summary = model.summary()\n",
    "    ### END SOLUTION\n",
    "    return model_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "part_c_check_text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Your function should return a summary view of your results. Check that it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ols_riskgender_on_bidvalue(fpa_data)) #we've wrapped this in a print statement to preserve the original statsmodels layout -- if you get a deprecation warning, that is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "correct_ols_type_summary",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance (ols_riskgender_on_bidvalue(fpa_data), sm.iolib.summary.Summary), \"checking your summary is output as a statsmodel summary object\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "part_c_inst_text_2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "2. Now, modify the ols_riskgender_on_bidvalue function to access the model's coefficients (parameters) and associated p-values, instead of printing out the entire summary view. For now, we won't worry about rounding. (1 point)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ols_riskgender_params",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def ols_riskgender_on_bidvalue(provided_data):\n",
    "    \n",
    "    \"\"\"\n",
    "    The easiest way to evaluate how subjects' risk attitudes and gender affect their bid/value ratio is to run an OLS linear\n",
    "    regression on your data dataframe. Use the statsmodels library to run an OLS linear regression, and this time return the\n",
    "    the coefficients and the p-values for your model.\n",
    "\n",
    "    \"\"\"\n",
    "    X = provided_data[['female', 'ra']]\n",
    "    # complete the function by assigning your Y, and fitting your model.\n",
    "    ### BEGIN SOLUTION\n",
    "    Y = provided_data['bv_ratio'] = provided_data['b'] / provided_data['v']\n",
    "    X = sm.add_constant(X)\n",
    "    model = sm.OLS(Y,X).fit()\n",
    "    model_params = model.params\n",
    "    pvals = model.pvalues\n",
    "    ### END SOLUTION\n",
    "    return model_params, pvals #we're returning a tuple of a series here -- pretty ugly, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "part_c_check_text_1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Your function should return a raw tuple of your results in pandas Series form. Check that it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_riskgender_on_bidvalue(fpa_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "correct_const_series",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"checking your return value is a tuple of type pandas series\"\n",
    "assert isinstance (ols_riskgender_on_bidvalue(fpa_data)[0], pd.core.series.Series)\n",
    "assert isinstance (ols_riskgender_on_bidvalue(fpa_data)[1], pd.core.series.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "correct_round_const",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"checking the value of 'const' for both values\"\n",
    "# Hidden tests\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert round(next(iter(ols_riskgender_on_bidvalue(fpa_data)[0])),2) == 1.06, \"Part C #2 const in first part of tuple differs\"\n",
    "assert round(next(iter(ols_riskgender_on_bidvalue(fpa_data)[1])),2) == 0.07, \"Part C #2 const in second part of tuple differs\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "part_c_inst_text_3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "3. Now, let's make our results more readable. Let's modify our function once again to this time create a dataframe that has the coefficients and p-values for the control variables and constant, **rounding to the nearest hundredth decimal**. (2 points)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ols_riskgender_df",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def ols_riskgender_on_bidvalue_df(provided_data):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function should use the results of the ols_riskgender_on_bidvalue function above to output a dataframe\n",
    "    that has the coefficients and p-values for the control variables and constant. \n",
    "    The dataframe should have the following columns: 'variable', 'coefficient', and 'p-value'\n",
    "    \n",
    "    \"\"\"\n",
    "    # define your parameters for your model and the p-values, then fill in the rest of the function below.\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    model_params = ols_riskgender_on_bidvalue(provided_data)[0]\n",
    "    pvals = ols_riskgender_on_bidvalue(provided_data)[1]\n",
    "    ### END SOLUTION\n",
    "    ols_model_df = pd.DataFrame(columns=['variable','coefficient','p-value'])\n",
    "    variables = ['const','ra','female']\n",
    "    ols_model_df['variable'] = variables\n",
    "    for variable in ols_model_df['variable']:\n",
    "        ### BEGIN SOLUTION\n",
    "        ols_model_df.loc[ols_model_df['variable']== variable,'coefficient'] = round(float(model_params[variable]),2)\n",
    "        ols_model_df.loc[ols_model_df['variable']==variable,'p-value'] = round(float(pvals[variable]),2)\n",
    "        ### END SOLUTION\n",
    "    return ols_model_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "part_c_check_text_2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Your function should return a dataframe of your results. Check that it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_riskgender_on_bidvalue_df(fpa_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "correct_ols_riskg_pval",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Part C: Check the dataframe outputs the correct p-values from OLS model\"\"\"\n",
    "# Hidden tests\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert ols_riskgender_on_bidvalue_df(fpa_data).iloc[2][2] == 0.41, \"Part C #3 female p-value differs\"\n",
    "assert ols_riskgender_on_bidvalue_df(fpa_data).iloc[1][2] == 0.89, \"Part C #3 risk aversion p-value differs\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "correct_ols_riskg_coeff",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Part C: Check the dataframe outputs the correct coefficients from OLS model\"\"\"\n",
    "# Hidden tests\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert ols_riskgender_on_bidvalue_df(fpa_data).iloc[2][1] == -0.21, \"Part C #3 female coefficient differs\"\n",
    "assert ols_riskgender_on_bidvalue_df(fpa_data).iloc[1][1] == -0.01, \"Part C #3 risk aversion coefficient differs\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "part_c_inst_text_4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "2. If you remove the risk attitudes variable from the model, does it have a significant effect on how gender contributes to bid/value ratios? Complete the ```ols_female_on_bidvalue``` function to assess this. Part of the function has already been completed for you. (3 points)\n",
    "\n",
    "**Round any calculations to the hundredth decimal. Do not use percentages.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ols_gender_bidval",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def ols_gender_on_bidvalue_df(provided_data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Complete the function that takes the provided data and creates a OLS model that determines the effect of \n",
    "    gender (using the female variable) on subjects' bid/value ratios. It should output (by filling in the missing values) \n",
    "    a dataframe that has the coefficients for the control variables and intercept.\n",
    "    \n",
    "    \"\"\"\n",
    "    # assign your X and Y variables, and define your parameters and pvalues. Then, fill in the rest of the function below.\n",
    "    ### BEGIN SOLUTION\n",
    "    X = provided_data['female']\n",
    "    Y = provided_data['bv_ratio'] = provided_data['b'] / provided_data['v']\n",
    "    X = sm.add_constant(X)\n",
    "    model = sm.OLS(Y,X).fit()\n",
    "    model_params = model.params\n",
    "    pvals = model.pvalues\n",
    "    ### END SOLUTION\n",
    "    ols_gender_model_df = pd.DataFrame(columns=['variable','coefficient','p-value'])\n",
    "    variables = ['const','female']\n",
    "    ols_gender_model_df['variable'] = variables\n",
    "\n",
    "    for variable in ols_gender_model_df['variable']:\n",
    "        ### BEGIN SOLUTION\n",
    "        ols_gender_model_df.loc[ols_gender_model_df['variable']==variable,'coefficient'] = round(float(model_params[variable]),2)\n",
    "        ols_gender_model_df.loc[ols_gender_model_df['variable']==variable,'p-value'] = round(float(pvals[variable]),2)\n",
    "        ### END SOLUTION\n",
    "    return ols_gender_model_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "part_c_check_text_3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Your function should return a dataframe with each of the variables and their completed coefficient and p-value for the OLS model. \n",
    "\n",
    "Check that it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_gender_on_bidvalue_df(fpa_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "correct_const_pval",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert ols_gender_on_bidvalue_df(fpa_data).iloc[0][2] == 0, \"checking the const p-value value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "correct_riskg_df_vals",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that the dataframe outputs the correct values from the OLS model\"\"\"\n",
    "# Hidden tests\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert ols_gender_on_bidvalue_df(fpa_data).iloc[0][1] == 0.98, \"Part C const coefficient differs\"\n",
    "assert ols_gender_on_bidvalue_df(fpa_data).iloc[1][1] == -0.21, \"Part C female coefficient differs\"\n",
    "assert ols_gender_on_bidvalue_df(fpa_data).iloc[1][2] == 0.4, \"Part C female p-value differs\"\n",
    "### END HIDDEN TESTS"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
